{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分类任务实战\n",
    "\n",
    "- 数据集构建：影评数据集进行情感分析（分类任务）\n",
    "- 词向量模型：加载训练好的词向量或者自己训练都可以\n",
    "- 循环网络模型：训练RNN模型进行识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/SentimentAnalysis2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN模型所需数据解读：\n",
    "\n",
    "![title](./img/1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pprint\n",
    "import logging\n",
    "import time\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载影评数据集，可以手动下载放到对应位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读进来的数据是已经转换成ID映射的，一般的数据读进来都是词语，都需要手动转换成ID映射的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 14,\n",
       " 22,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 973,\n",
       " 1622,\n",
       " 1385,\n",
       " 65,\n",
       " 458,\n",
       " 4468,\n",
       " 66,\n",
       " 3941,\n",
       " 4,\n",
       " 173,\n",
       " 36,\n",
       " 256,\n",
       " 5,\n",
       " 25,\n",
       " 100,\n",
       " 43,\n",
       " 838,\n",
       " 112,\n",
       " 50,\n",
       " 670,\n",
       " 22665,\n",
       " 9,\n",
       " 35,\n",
       " 480,\n",
       " 284,\n",
       " 5,\n",
       " 150,\n",
       " 4,\n",
       " 172,\n",
       " 112,\n",
       " 167,\n",
       " 21631,\n",
       " 336,\n",
       " 385,\n",
       " 39,\n",
       " 4,\n",
       " 172,\n",
       " 4536,\n",
       " 1111,\n",
       " 17,\n",
       " 546,\n",
       " 38,\n",
       " 13,\n",
       " 447,\n",
       " 4,\n",
       " 192,\n",
       " 50,\n",
       " 16,\n",
       " 6,\n",
       " 147,\n",
       " 2025,\n",
       " 19,\n",
       " 14,\n",
       " 22,\n",
       " 4,\n",
       " 1920,\n",
       " 4613,\n",
       " 469,\n",
       " 4,\n",
       " 22,\n",
       " 71,\n",
       " 87,\n",
       " 12,\n",
       " 16,\n",
       " 43,\n",
       " 530,\n",
       " 38,\n",
       " 76,\n",
       " 15,\n",
       " 13,\n",
       " 1247,\n",
       " 4,\n",
       " 22,\n",
       " 17,\n",
       " 515,\n",
       " 17,\n",
       " 12,\n",
       " 16,\n",
       " 626,\n",
       " 18,\n",
       " 19193,\n",
       " 5,\n",
       " 62,\n",
       " 386,\n",
       " 12,\n",
       " 8,\n",
       " 316,\n",
       " 8,\n",
       " 106,\n",
       " 5,\n",
       " 4,\n",
       " 2223,\n",
       " 5244,\n",
       " 16,\n",
       " 480,\n",
       " 66,\n",
       " 3785,\n",
       " 33,\n",
       " 4,\n",
       " 130,\n",
       " 12,\n",
       " 16,\n",
       " 38,\n",
       " 619,\n",
       " 5,\n",
       " 25,\n",
       " 124,\n",
       " 51,\n",
       " 36,\n",
       " 135,\n",
       " 48,\n",
       " 25,\n",
       " 1415,\n",
       " 33,\n",
       " 6,\n",
       " 22,\n",
       " 12,\n",
       " 215,\n",
       " 28,\n",
       " 77,\n",
       " 52,\n",
       " 5,\n",
       " 14,\n",
       " 407,\n",
       " 16,\n",
       " 82,\n",
       " 10311,\n",
       " 8,\n",
       " 4,\n",
       " 107,\n",
       " 117,\n",
       " 5952,\n",
       " 15,\n",
       " 256,\n",
       " 4,\n",
       " 31050,\n",
       " 7,\n",
       " 3766,\n",
       " 5,\n",
       " 723,\n",
       " 36,\n",
       " 71,\n",
       " 43,\n",
       " 530,\n",
       " 476,\n",
       " 26,\n",
       " 400,\n",
       " 317,\n",
       " 46,\n",
       " 7,\n",
       " 4,\n",
       " 12118,\n",
       " 1029,\n",
       " 13,\n",
       " 104,\n",
       " 88,\n",
       " 4,\n",
       " 381,\n",
       " 15,\n",
       " 297,\n",
       " 98,\n",
       " 32,\n",
       " 2071,\n",
       " 56,\n",
       " 26,\n",
       " 141,\n",
       " 6,\n",
       " 194,\n",
       " 7486,\n",
       " 18,\n",
       " 4,\n",
       " 226,\n",
       " 22,\n",
       " 21,\n",
       " 134,\n",
       " 476,\n",
       " 26,\n",
       " 480,\n",
       " 5,\n",
       " 144,\n",
       " 30,\n",
       " 5535,\n",
       " 18,\n",
       " 51,\n",
       " 36,\n",
       " 28,\n",
       " 224,\n",
       " 92,\n",
       " 25,\n",
       " 104,\n",
       " 4,\n",
       " 226,\n",
       " 65,\n",
       " 16,\n",
       " 38,\n",
       " 1334,\n",
       " 88,\n",
       " 12,\n",
       " 16,\n",
       " 283,\n",
       " 5,\n",
       " 16,\n",
       " 4472,\n",
       " 113,\n",
       " 103,\n",
       " 32,\n",
       " 15,\n",
       " 16,\n",
       " 5345,\n",
       " 19,\n",
       " 178,\n",
       " 32]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词和ID的映射表，空出来3个的目的是加上特殊字符"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_word2idx = tf.keras.datasets.imdb.get_word_index()\n",
    "word2idx = {w: i+3 for w, i in _word2idx.items()}\n",
    "word2idx['<pad>'] = 0 # 填充字符\n",
    "word2idx['<start>'] = 1 # 开始索引\n",
    "word2idx['<unk>'] = 2 # 没有对应关系的词\n",
    "idx2word = {i: w for w, i in word2idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按文本长度大小进行排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_len(x, y):\n",
    "    x, y = np.asarray(x), np.asarray(y)\n",
    "    idx = sorted(range(len(x)), key=lambda i: len(x[i]))\n",
    "    return x[idx], y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将中间结果保存到本地，万一程序崩了还能重来，保存的是文本数据，不是ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = sort_by_len(x_train, y_train)\n",
    "x_test, y_test = sort_by_len(x_test, y_test)\n",
    "\n",
    "def write_file(f_path, xs, ys):\n",
    "    with open(f_path, 'w',encoding='utf-8') as f:\n",
    "        for x, y in zip(xs, ys):\n",
    "            f.write(str(y)+'\\t'+' '.join([idx2word[i] for i in x][1:])+'\\n')\n",
    "\n",
    "write_file('./data/train.txt', x_train, y_train)\n",
    "write_file('./data/test.txt', x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建语料表，基于词频来进行统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 20598\n"
     ]
    }
   ],
   "source": [
    "counter = Counter()\n",
    "with open('./data/train.txt',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()\n",
    "        label, words = line.split('\\t')\n",
    "        words = words.split(' ')\n",
    "        counter.update(words)\n",
    "\n",
    "words = ['<pad>'] + [w for w, freq in counter.most_common() if freq >= 10]\n",
    "print('Vocab Size:', len(words))\n",
    "\n",
    "Path('./vocab').mkdir(exist_ok=True)\n",
    "\n",
    "with open('./vocab/word.txt', 'w',encoding='utf-8') as f:\n",
    "    for w in words:\n",
    "        f.write(w+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 得到新的word2id映射表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "with open('./vocab/word.txt',encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.rstrip()\n",
    "        word2idx[line] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## embedding层\n",
    "- 词向量\n",
    "- 可以基于网络来训练，也可以直接加载别人训练好的，一般都是加载预训练模型\n",
    "- 这里有一些常用的：https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![title](./img/SentimentAnalysis3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- At line 0\n",
      "- At line 100000\n",
      "- At line 200000\n",
      "- At line 300000\n"
     ]
    }
   ],
   "source": [
    "# 做了一个大表，里面有20598个不同的词，【20599*50】\n",
    "embedding = np.zeros((len(word2idx)+1, 50)) # + 1 表示如果不在语料表中，就都是unknow\n",
    "\n",
    "with open('./data/glove.6B.50d.txt',encoding='utf-8') as f: # 下载好的\n",
    "    count = 0\n",
    "    for i, line in enumerate(f):\n",
    "        if i % 100000 == 0:\n",
    "            print('- At line {}'.format(i)) # 打印处理了多少数据\n",
    "        line = line.rstrip()\n",
    "        sp = line.split(' ')\n",
    "        word, vec = sp[0], sp[1:]\n",
    "        if word in word2idx:\n",
    "            count += 1\n",
    "            embedding[word2idx[word]] = np.asarray(vec, dtype='float32') # 将词转换成对应的向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在已经得到每个词索引所对应的向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19676 / 20598] words have found pre-trained values\n",
      "Saved ./vocab/word.npy\n"
     ]
    }
   ],
   "source": [
    "print(\"[%d / %d] words have found pre-trained values\"%(count, len(word2idx)))\n",
    "np.save('./vocab/word.npy', embedding)\n",
    "print('Saved ./vocab/word.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建训练数据\n",
    "\n",
    "- 注意所有的输入样本必须都是相同shape（文本长度，词向量维度等）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据生成器\n",
    "- tf.data.Dataset.from_tensor_slices(tensor)：将tensor沿其第一个维度切片，返回一个含有N个样本的数据集，这样做的问题就是需要将整个数据集整体传入，然后切片建立数据集类对象，比较占内存。\n",
    "\n",
    "- tf.data.Dataset.from_generator(data_generator,output_data_type,output_data_shape)：从一个生成器中不断读取样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(f_path, params):\n",
    "    with open(f_path,encoding='utf-8') as f:\n",
    "        print('Reading', f_path)\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            label, text = line.split('\\t')\n",
    "            text = text.split(' ')\n",
    "            x = [params['word2idx'].get(w, len(word2idx)) for w in text]# 得到当前词所对应的ID\n",
    "            if len(x) >= params['max_len']:# 截断操作\n",
    "                x = x[:params['max_len']]\n",
    "            else:\n",
    "                x += [0] * (params['max_len'] - len(x))# 补齐操作\n",
    "            y = int(label)\n",
    "            yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(is_training, params):\n",
    "    _shapes = ([params['max_len']], ())\n",
    "    _types = (tf.int32, tf.int32)\n",
    "\n",
    "    if is_training:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            lambda: data_generator(params['train_path'], params),\n",
    "            output_shapes = _shapes,\n",
    "            output_types = _types,)\n",
    "        ds = ds.shuffle(params['num_samples'])\n",
    "        ds = ds.batch(params['batch_size'])\n",
    "        # 设置缓存序列，根据可用的CPU动态设置并行调用的数量，说白了就是加速\n",
    "        ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    else:\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            lambda: data_generator(params['test_path'], params),\n",
    "            output_shapes = _shapes,\n",
    "            output_types = _types,)\n",
    "        ds = ds.batch(params['batch_size'])\n",
    "        ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义网络模型\n",
    "- 定义好都有哪些层\n",
    "- 前向传播走一遍就行了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "embedding_lookup的作用："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/SentimentAnalysis5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/SentimentAnalysis16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.embedding = tf.Variable(np.load('./vocab/word.npy'),\n",
    "                                     dtype=tf.float32,\n",
    "                                     name='pretrained_embedding',\n",
    "                                     trainable=False,)\n",
    "\n",
    "        self.drop1 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.drop2 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.drop3 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "\n",
    "        self.rnn1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units'], return_sequences=True))\n",
    "        self.rnn2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units'], return_sequences=True))\n",
    "        self.rnn3 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units'], return_sequences=False))\n",
    "\n",
    "        self.drop_fc = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.fc = tf.keras.layers.Dense(2*params['rnn_units'], tf.nn.elu)\n",
    "\n",
    "        self.out_linear = tf.keras.layers.Dense(2)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if inputs.dtype != tf.int32:\n",
    "            inputs = tf.cast(inputs, tf.int32)\n",
    "    \n",
    "        batch_sz = tf.shape(inputs)[0]\n",
    "        rnn_units = 2*params['rnn_units']\n",
    "\n",
    "        x = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "        \n",
    "        x = self.drop1(x, training=training)\n",
    "        x = self.rnn1(x)\n",
    "\n",
    "        x = self.drop2(x, training=training)\n",
    "        x = self.rnn2(x)\n",
    "\n",
    "        x = self.drop3(x, training=training)\n",
    "        x = self.rnn3(x)\n",
    "\n",
    "        x = self.drop_fc(x, training=training)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = self.out_linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 二版本：速度会更快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.embedding = tf.Variable(np.load('./vocab/word.npy'),\n",
    "                                     dtype=tf.float32,\n",
    "                                     name='pretrained_embedding',\n",
    "                                     trainable=False,)\n",
    "\n",
    "        self.drop1 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.drop2 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.drop3 = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "\n",
    "        self.rnn1 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units'], return_sequences=True))\n",
    "        self.rnn2 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units'], return_sequences=True))\n",
    "        self.rnn3 = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(params['rnn_units'], return_sequences=True))\n",
    "\n",
    "        self.drop_fc = tf.keras.layers.Dropout(params['dropout_rate'])\n",
    "        self.fc = tf.keras.layers.Dense(2*params['rnn_units'], tf.nn.elu)\n",
    "\n",
    "        self.out_linear = tf.keras.layers.Dense(2)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        if inputs.dtype != tf.int32:\n",
    "            inputs = tf.cast(inputs, tf.int32)\n",
    "    \n",
    "        batch_sz = tf.shape(inputs)[0]\n",
    "        rnn_units = 2*params['rnn_units']\n",
    "\n",
    "        x = tf.nn.embedding_lookup(self.embedding, inputs)\n",
    "        \n",
    "        x = tf.reshape(x, (batch_sz*10*10, 10, 50))\n",
    "        x = self.drop1(x, training=training)\n",
    "        x = self.rnn1(x)\n",
    "        x = tf.reduce_max(x, 1)\n",
    "\n",
    "        x = tf.reshape(x, (batch_sz*10, 10, rnn_units))\n",
    "        x = self.drop2(x, training=training)\n",
    "        x = self.rnn2(x)\n",
    "        x = tf.reduce_max(x, 1)\n",
    "\n",
    "        x = tf.reshape(x, (batch_sz, 10, rnn_units))\n",
    "        x = self.drop3(x, training=training)\n",
    "        x = self.rnn3(x)\n",
    "        x = tf.reduce_max(x, 1)\n",
    "\n",
    "        x = self.drop_fc(x, training=training)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = self.out_linear(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "  'vocab_path': './vocab/word.txt',\n",
    "  'train_path': './data/train.txt',\n",
    "  'test_path': './data/test.txt',\n",
    "  'num_samples': 25000,\n",
    "  'num_labels': 2,\n",
    "  'batch_size': 32,\n",
    "  'max_len': 1000,\n",
    "  'rnn_units': 200,\n",
    "  'dropout_rate': 0.2,\n",
    "  'clip_norm': 10.,\n",
    "  'num_patience': 3,\n",
    "  'lr': 3e-4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用来判断进行提前停止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_descending(history: list):\n",
    "    history = history[-(params['num_patience']+1):]\n",
    "    for i in range(1, len(history)):\n",
    "        if history[i-1] <= history[i]:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {}\n",
    "with open(params['vocab_path'],encoding='utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.rstrip()\n",
    "        word2idx[line] = i\n",
    "params['word2idx'] = word2idx\n",
    "params['vocab_size'] = len(word2idx) + 1\n",
    "\n",
    "model = Model(params)\n",
    "model.build(input_shape=(None, None))# 设置输入的大小，或者fit时候也能自动找到\n",
    "# pprint.pprint([(v.name, v.shape) for v in model.trainable_variables])\n",
    "\n",
    "# 链接：https://tensorflow.google.cn/api_docs/python/tf/keras/optimizers/schedules/ExponentialDecay?version=stable\n",
    "# return initial_learning_rate * decay_rate ^ (step / decay_steps)\n",
    "decay_lr = tf.optimizers.schedules.ExponentialDecay(params['lr'], 1000, 0.95)# 相当于加了一个指数衰减函数\n",
    "optim = tf.optimizers.Adam(params['lr'])\n",
    "global_step = 0\n",
    "\n",
    "history_acc = []\n",
    "best_acc = .0\n",
    "\n",
    "t0 = time.time()\n",
    "logger = logging.getLogger('tensorflow')\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./data/train.txt\n",
      "INFO:tensorflow:Step 0 | Loss: 0.6942 | Spent: 9.0 secs | LR: 0.000300\n",
      "INFO:tensorflow:Step 50 | Loss: 0.6793 | Spent: 113.2 secs | LR: 0.000299\n",
      "INFO:tensorflow:Step 100 | Loss: 0.6809 | Spent: 125.2 secs | LR: 0.000298\n",
      "INFO:tensorflow:Step 150 | Loss: 0.4341 | Spent: 118.5 secs | LR: 0.000298\n",
      "INFO:tensorflow:Step 200 | Loss: 0.5307 | Spent: 138.3 secs | LR: 0.000297\n",
      "INFO:tensorflow:Step 250 | Loss: 0.6829 | Spent: 127.1 secs | LR: 0.000296\n",
      "INFO:tensorflow:Step 300 | Loss: 0.4500 | Spent: 125.9 secs | LR: 0.000295\n",
      "INFO:tensorflow:Step 350 | Loss: 0.5058 | Spent: 114.3 secs | LR: 0.000295\n",
      "INFO:tensorflow:Step 400 | Loss: 0.4795 | Spent: 107.8 secs | LR: 0.000294\n",
      "INFO:tensorflow:Step 450 | Loss: 0.5104 | Spent: 107.9 secs | LR: 0.000293\n",
      "INFO:tensorflow:Step 500 | Loss: 0.4912 | Spent: 102.0 secs | LR: 0.000292\n",
      "INFO:tensorflow:Step 550 | Loss: 0.5989 | Spent: 105.6 secs | LR: 0.000292\n",
      "INFO:tensorflow:Step 600 | Loss: 0.2602 | Spent: 103.1 secs | LR: 0.000291\n",
      "INFO:tensorflow:Step 650 | Loss: 0.3271 | Spent: 101.4 secs | LR: 0.000290\n",
      "INFO:tensorflow:Step 700 | Loss: 0.4834 | Spent: 107.7 secs | LR: 0.000289\n",
      "INFO:tensorflow:Step 750 | Loss: 0.3278 | Spent: 106.1 secs | LR: 0.000289\n",
      "Reading ./data/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.806\n",
      "INFO:tensorflow:Best Accuracy: 0.806\n",
      "Reading ./data/train.txt\n",
      "INFO:tensorflow:Step 800 | Loss: 0.6959 | Spent: 894.3 secs | LR: 0.000288\n",
      "INFO:tensorflow:Step 850 | Loss: 0.3919 | Spent: 104.5 secs | LR: 0.000287\n",
      "INFO:tensorflow:Step 900 | Loss: 0.4799 | Spent: 101.1 secs | LR: 0.000286\n",
      "INFO:tensorflow:Step 950 | Loss: 0.4031 | Spent: 105.5 secs | LR: 0.000286\n",
      "INFO:tensorflow:Step 1000 | Loss: 0.3150 | Spent: 108.2 secs | LR: 0.000285\n",
      "INFO:tensorflow:Step 1050 | Loss: 0.3765 | Spent: 106.9 secs | LR: 0.000284\n",
      "INFO:tensorflow:Step 1100 | Loss: 0.4720 | Spent: 103.5 secs | LR: 0.000284\n",
      "INFO:tensorflow:Step 1150 | Loss: 0.4035 | Spent: 111.3 secs | LR: 0.000283\n",
      "INFO:tensorflow:Step 1200 | Loss: 0.3921 | Spent: 101.5 secs | LR: 0.000282\n",
      "INFO:tensorflow:Step 1250 | Loss: 0.3339 | Spent: 111.4 secs | LR: 0.000281\n",
      "INFO:tensorflow:Step 1300 | Loss: 0.2717 | Spent: 96.9 secs | LR: 0.000281\n",
      "INFO:tensorflow:Step 1350 | Loss: 0.3750 | Spent: 97.3 secs | LR: 0.000280\n",
      "INFO:tensorflow:Step 1400 | Loss: 0.5280 | Spent: 97.0 secs | LR: 0.000279\n",
      "INFO:tensorflow:Step 1450 | Loss: 0.4056 | Spent: 95.3 secs | LR: 0.000278\n",
      "INFO:tensorflow:Step 1500 | Loss: 0.5615 | Spent: 95.2 secs | LR: 0.000278\n",
      "INFO:tensorflow:Step 1550 | Loss: 0.3269 | Spent: 95.1 secs | LR: 0.000277\n",
      "Reading ./data/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.817\n",
      "INFO:tensorflow:Best Accuracy: 0.817\n",
      "Reading ./data/train.txt\n",
      "INFO:tensorflow:Step 1600 | Loss: 0.5400 | Spent: 781.1 secs | LR: 0.000276\n",
      "INFO:tensorflow:Step 1650 | Loss: 0.3094 | Spent: 95.5 secs | LR: 0.000276\n",
      "INFO:tensorflow:Step 1700 | Loss: 0.4595 | Spent: 95.3 secs | LR: 0.000275\n",
      "INFO:tensorflow:Step 1750 | Loss: 0.2278 | Spent: 96.0 secs | LR: 0.000274\n",
      "INFO:tensorflow:Step 1800 | Loss: 0.4701 | Spent: 95.6 secs | LR: 0.000274\n",
      "INFO:tensorflow:Step 1850 | Loss: 0.3088 | Spent: 94.9 secs | LR: 0.000273\n",
      "INFO:tensorflow:Step 1900 | Loss: 0.3100 | Spent: 95.2 secs | LR: 0.000272\n",
      "INFO:tensorflow:Step 1950 | Loss: 0.4502 | Spent: 95.3 secs | LR: 0.000271\n",
      "INFO:tensorflow:Step 2000 | Loss: 0.7268 | Spent: 94.8 secs | LR: 0.000271\n",
      "INFO:tensorflow:Step 2050 | Loss: 0.3401 | Spent: 95.0 secs | LR: 0.000270\n",
      "INFO:tensorflow:Step 2100 | Loss: 0.3794 | Spent: 95.0 secs | LR: 0.000269\n",
      "INFO:tensorflow:Step 2150 | Loss: 0.4237 | Spent: 95.0 secs | LR: 0.000269\n",
      "INFO:tensorflow:Step 2200 | Loss: 0.4280 | Spent: 94.8 secs | LR: 0.000268\n",
      "INFO:tensorflow:Step 2250 | Loss: 0.3519 | Spent: 94.9 secs | LR: 0.000267\n",
      "INFO:tensorflow:Step 2300 | Loss: 0.3171 | Spent: 95.0 secs | LR: 0.000267\n",
      "Reading ./data/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.847\n",
      "INFO:tensorflow:Best Accuracy: 0.847\n",
      "Reading ./data/train.txt\n",
      "INFO:tensorflow:Step 2350 | Loss: 0.2910 | Spent: 781.8 secs | LR: 0.000266\n",
      "INFO:tensorflow:Step 2400 | Loss: 0.5251 | Spent: 95.2 secs | LR: 0.000265\n",
      "INFO:tensorflow:Step 2450 | Loss: 0.3745 | Spent: 94.9 secs | LR: 0.000265\n",
      "INFO:tensorflow:Step 2500 | Loss: 0.2530 | Spent: 95.5 secs | LR: 0.000264\n",
      "INFO:tensorflow:Step 2550 | Loss: 0.5215 | Spent: 95.3 secs | LR: 0.000263\n",
      "INFO:tensorflow:Step 2600 | Loss: 0.5640 | Spent: 95.5 secs | LR: 0.000263\n",
      "INFO:tensorflow:Step 2650 | Loss: 0.3690 | Spent: 95.4 secs | LR: 0.000262\n",
      "INFO:tensorflow:Step 2700 | Loss: 0.3616 | Spent: 95.2 secs | LR: 0.000261\n",
      "INFO:tensorflow:Step 2750 | Loss: 0.4644 | Spent: 95.3 secs | LR: 0.000261\n",
      "INFO:tensorflow:Step 2800 | Loss: 0.3359 | Spent: 95.2 secs | LR: 0.000260\n",
      "INFO:tensorflow:Step 2850 | Loss: 0.3421 | Spent: 95.1 secs | LR: 0.000259\n",
      "INFO:tensorflow:Step 2900 | Loss: 0.3085 | Spent: 95.2 secs | LR: 0.000259\n",
      "INFO:tensorflow:Step 2950 | Loss: 0.3213 | Spent: 95.2 secs | LR: 0.000258\n",
      "INFO:tensorflow:Step 3000 | Loss: 0.2251 | Spent: 95.5 secs | LR: 0.000257\n",
      "INFO:tensorflow:Step 3050 | Loss: 0.4085 | Spent: 95.3 secs | LR: 0.000257\n",
      "INFO:tensorflow:Step 3100 | Loss: 0.1690 | Spent: 95.1 secs | LR: 0.000256\n",
      "Reading ./data/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.854\n",
      "INFO:tensorflow:Best Accuracy: 0.854\n",
      "Reading ./data/train.txt\n",
      "INFO:tensorflow:Step 3150 | Loss: 0.4990 | Spent: 784.0 secs | LR: 0.000255\n",
      "INFO:tensorflow:Step 3200 | Loss: 0.3214 | Spent: 96.0 secs | LR: 0.000255\n",
      "INFO:tensorflow:Step 3250 | Loss: 0.3742 | Spent: 94.9 secs | LR: 0.000254\n",
      "INFO:tensorflow:Step 3300 | Loss: 0.2260 | Spent: 95.0 secs | LR: 0.000253\n",
      "INFO:tensorflow:Step 3350 | Loss: 0.4265 | Spent: 95.0 secs | LR: 0.000253\n",
      "INFO:tensorflow:Step 3400 | Loss: 0.3187 | Spent: 95.0 secs | LR: 0.000252\n",
      "INFO:tensorflow:Step 3450 | Loss: 0.2879 | Spent: 94.9 secs | LR: 0.000251\n",
      "INFO:tensorflow:Step 3500 | Loss: 0.3093 | Spent: 95.1 secs | LR: 0.000251\n",
      "INFO:tensorflow:Step 3550 | Loss: 0.3978 | Spent: 94.9 secs | LR: 0.000250\n",
      "INFO:tensorflow:Step 3600 | Loss: 0.3394 | Spent: 95.1 secs | LR: 0.000249\n",
      "INFO:tensorflow:Step 3650 | Loss: 0.2536 | Spent: 95.2 secs | LR: 0.000249\n",
      "INFO:tensorflow:Step 3700 | Loss: 0.5181 | Spent: 94.8 secs | LR: 0.000248\n",
      "INFO:tensorflow:Step 3750 | Loss: 0.4218 | Spent: 95.1 secs | LR: 0.000248\n",
      "INFO:tensorflow:Step 3800 | Loss: 0.3781 | Spent: 94.9 secs | LR: 0.000247\n",
      "INFO:tensorflow:Step 3850 | Loss: 0.3272 | Spent: 95.0 secs | LR: 0.000246\n",
      "INFO:tensorflow:Step 3900 | Loss: 0.3535 | Spent: 95.2 secs | LR: 0.000246\n",
      "Reading ./data/test.txt\n",
      "INFO:tensorflow:Evaluation: Testing Accuracy: 0.826\n",
      "INFO:tensorflow:Best Accuracy: 0.854\n",
      "Reading ./data/train.txt\n",
      "INFO:tensorflow:Step 3950 | Loss: 0.4422 | Spent: 783.4 secs | LR: 0.000245\n",
      "INFO:tensorflow:Step 4000 | Loss: 0.3389 | Spent: 95.1 secs | LR: 0.000244\n",
      "INFO:tensorflow:Step 4050 | Loss: 0.3955 | Spent: 94.9 secs | LR: 0.000244\n",
      "INFO:tensorflow:Step 4100 | Loss: 0.3539 | Spent: 94.9 secs | LR: 0.000243\n",
      "INFO:tensorflow:Step 4150 | Loss: 0.3030 | Spent: 95.2 secs | LR: 0.000242\n",
      "INFO:tensorflow:Step 4200 | Loss: 0.1807 | Spent: 103.7 secs | LR: 0.000242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c9e35d3d5b54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m# 梯度带，记录所有在上下文中的操作，并且通过调用.gradient()获得任何上下文中计算得出的张量的梯度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_softmax_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-f160e8e28ef4>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_sz\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce_max\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m     \u001b[1;31m# Applies the same workaround as in `RNN.__call__`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, training, mask, initial_state, constants)\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[0mforward_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackward_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 643\u001b[1;33m       y = self.forward_layer(forward_inputs,\n\u001b[0m\u001b[0;32m    644\u001b[0m                              initial_state=forward_state, **kwargs)\n\u001b[0;32m    645\u001b[0m       y_rev = self.backward_layer(backward_inputs,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 659\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m     \u001b[1;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    983\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_auto_cast_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 985\u001b[1;33m           \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    986\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    987\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[0;32m   1177\u001b[0m               **gpu_lstm_kwargs)\n\u001b[0;32m   1178\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1179\u001b[1;33m           last_output, outputs, new_h, new_c, runtime = standard_lstm(\n\u001b[0m\u001b[0;32m   1180\u001b[0m               **normal_lstm_kwargs)\n\u001b[0;32m   1181\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mstandard_lstm\u001b[1;34m(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask, time_major, go_backwards, sequence_lengths, zero_output_for_mask)\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1305\u001b[1;33m   last_output, outputs, new_states = K.rnn(\n\u001b[0m\u001b[0;32m   1306\u001b[0m       \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m       \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minit_h\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minit_c\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mrnn\u001b[1;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_ta_t\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4358\u001b[1;33m       final_outputs = control_flow_ops.while_loop(\n\u001b[0m\u001b[0;32m   4359\u001b[0m           \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0m_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4360\u001b[0m           \u001b[0mloop_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[1;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[0;32m   2733\u001b[0m                                               list(loop_vars))\n\u001b[0;32m   2734\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2735\u001b[1;33m         \u001b[0mloop_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2736\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2737\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[0;32m   4342\u001b[0m         \u001b[0mcurrent_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_ta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4343\u001b[0m         \u001b[0mcurrent_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4344\u001b[1;33m         output, new_states = step_function(current_input,\n\u001b[0m\u001b[0;32m   4345\u001b[0m                                            tuple(states) + tuple(constants))\n\u001b[0;32m   4346\u001b[0m         \u001b[0mflat_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent_v2.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(cell_inputs, cell_states)\u001b[0m\n\u001b[0;32m   1289\u001b[0m     \u001b[0mc_tm1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcell_states\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# previous carry state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1291\u001b[1;33m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1292\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_tm1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_kernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mdot\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m   1829\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1830\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1831\u001b[1;33m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1832\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36mmatmul\u001b[1;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[0;32m   3251\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3252\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3253\u001b[1;33m       return gen_math_ops.mat_mul(\n\u001b[0m\u001b[0;32m   3254\u001b[0m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0;32m   3255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   5616\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5617\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5618\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   5619\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MatMul\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5620\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_a\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"transpose_b\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time1 = datetime.datetime.now()\n",
    "\n",
    "while True:\n",
    "  # 训练模型\n",
    "    for texts, labels in dataset(is_training=True, params=params):\n",
    "        with tf.GradientTape() as tape:# 梯度带，记录所有在上下文中的操作，并且通过调用.gradient()获得任何上下文中计算得出的张量的梯度\n",
    "            logits = model(texts, training=True)\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        optim.lr.assign(decay_lr(global_step))\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        grads, _ = tf.clip_by_global_norm(grads, params['clip_norm']) # 将梯度限制一下，有的时候回更新太猛，防止过拟合\n",
    "        optim.apply_gradients(zip(grads, model.trainable_variables))# 更新梯度\n",
    "\n",
    "        if global_step % 50 == 0:\n",
    "            logger.info(\"Step {} | Loss: {:.4f} | Spent: {:.1f} secs | LR: {:.6f}\".format(\n",
    "                global_step, loss.numpy().item(), time.time()-t0, optim.lr.numpy().item()))\n",
    "            t0 = time.time()\n",
    "        global_step += 1\n",
    "\n",
    "    # 验证集效果\n",
    "    m = tf.keras.metrics.Accuracy()\n",
    "\n",
    "    for texts, labels in dataset(is_training=False, params=params):\n",
    "        logits = model(texts, training=False)\n",
    "        y_pred = tf.argmax(logits, axis=-1)\n",
    "        m.update_state(y_true=labels, y_pred=y_pred)\n",
    "    \n",
    "    acc = m.result().numpy()\n",
    "    logger.info(\"Evaluation: Testing Accuracy: {:.3f}\".format(acc))\n",
    "    history_acc.append(acc)\n",
    "\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "    logger.info(\"Best Accuracy: {:.3f}\".format(best_acc))\n",
    "\n",
    "    if len(history_acc) > params['num_patience'] and is_descending(history_acc):\n",
    "        logger.info(\"Testing Accuracy not improved over {} epochs, Early Stop\".format(params['num_patience']))\n",
    "        break\n",
    "        \n",
    "print(datetime.datetime.now() - time1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
